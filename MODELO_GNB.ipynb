{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MODELO_GNB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/femIA2020/FEM_IA/blob/master/MODELO_GNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmiBO6Jtf7-"
      },
      "source": [
        "# Open csv \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "#sentences = pd.read_csv(\"/content/drive/My Drive/folder_0/Projecto/Model/sentencias_dataframe.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6X0aOIUESCV"
      },
      "source": [
        "sentences.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2HBBIIGKg_i"
      },
      "source": [
        "# There are missing values in dataframe , column 'CORPUS'\n",
        "nan_values = sentences [sentences [\"CORPUS\"].isnull() == True ] \n",
        "nan_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq69YW4_Nhcy"
      },
      "source": [
        " #Only take values that are not missing and check again shape \n",
        "sentences = sentences[sentences['CORPUS'].notnull()]\n",
        "sentences.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1gJ_oQtHbZA"
      },
      "source": [
        "sentences['PUNTUACIÓN'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXQ0ViZFQP2E"
      },
      "source": [
        "# Add new column : PG if punctuation is greater than 6 then is a sentence that contains PERSPECTIVA DE GÉNERO and if not then is FALSE. \n",
        "sentences['PG'] = np.where(sentences['PUNTUACIÓN'] > 6, True, False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULAZH4L3Kuo9"
      },
      "source": [
        "#DATAVIZ : ASUNTO/TEMA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud().generate(\" \".join(sentences['ASUNTO/TEMA']))\n",
        "\n",
        "plt.imshow(wordcloud,interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjJbXaI_kNUM"
      },
      "source": [
        "#EDA : ASUNTO/TEMA\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,20)\n",
        "sentences['ASUNTO/TEMA'].value_counts().plot(kind ='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yqi-fbyLPnI"
      },
      "source": [
        "#EDA : INSTANCIA\n",
        "sentences['INSTANCIA'].value_counts().plot(kind ='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLjohfs3hFTE"
      },
      "source": [
        "#PUNTUACION \n",
        "sentences['PUNTUACIÓN'].plot.box(title=\"Puntuación de perspectiva de género\", grid=True);\n",
        "\n",
        "plt.show(block=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR7onQ2_tcNR"
      },
      "source": [
        "import seaborn as sn\n",
        "ax = sn.boxplot(sentences['PUNTUACIÓN'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km9q5kZhw0Hm"
      },
      "source": [
        "# Pra ver todo el corpus: aun hay nombres propios, acentros, caracteres especiales como asteriscos y , quitar nombre de ACsnumeros, qué pasa con los NAN, quitar numeros\n",
        "#from google.colab.data_table import DataTable\n",
        "#%load_ext google.colab.data_table\n",
        "#DataTable(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROTQhaYxSSbW"
      },
      "source": [
        "sentences['CORPUS'] = sentences['CORPUS'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-efP_ayTaTP"
      },
      "source": [
        "# Remove punctuation : checar, se están pasando acentos\n",
        "\n",
        "PUNCT_TO_REMOVE = string.punctuation #String punctuation default symbols:!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "sentences[\"text_wo_punct\"] = sentences[\"CORPUS\"].apply(lambda text: remove_punctuation(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsvEk2ULUawK"
      },
      "source": [
        "# remove stop words : #ACA FALTAN MÁS STOPWORDS COMO DOS O SI \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS = set(stopwords.words('spanish'))\n",
        "STOPWORDS = list(STOPWORDS) + [\"dos\",\"mil\",\" ”\",\" “\",\" “\",\"i\"] #here we can add more stopwords to eliminate\n",
        "\n",
        "def remove_stopwords(text):\n",
        "   return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "sentences[\"text_wo_stop\"] = sentences[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMcJHB8WDT9"
      },
      "source": [
        "#Stemming : no se ve muy bien \n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "sentences[\"text_stemmed\"] = sentences[\"text_wo_stop\"].apply(lambda text: stem_words(text))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYKUbu1jD1sD"
      },
      "source": [
        "#test/train\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "X_unscaled = pd.DataFrame(sentences)\n",
        "#X_unscaled.drop ('score', axis = 1 , inplace = True)\n",
        "\n",
        "y = pd.Series(sentences.PG, name = \"PG\")\n",
        "y=y.astype('int')\n",
        "\n",
        "#Split dataset : 80% training set & 20% test set \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_unscaled,y, test_size=0.2, random_state=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgp8qO83GBqc"
      },
      "source": [
        "# print lenght of train & test\n",
        "ds_train = X_train\n",
        "ds_test = X_test\n",
        "print(\"Number of records in Train data set\",len(ds_train.index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myFWDFaYGY8e"
      },
      "source": [
        "# Create dictionary \n",
        "\n",
        "corpus  = ds_train['text_stemmed'] #acá el corpus debería ser lemma o stemm\n",
        "\n",
        "uniqueWords = {}\n",
        "for text in corpus:\n",
        "    for word in text.split():\n",
        "        if(word in uniqueWords.keys()):\n",
        "            uniqueWords[word] += 1\n",
        "        else:\n",
        "            uniqueWords[word] = 1\n",
        "            \n",
        "#Convert dictionary to dataFrame\n",
        "uniqueWords = pd.DataFrame.from_dict(uniqueWords,orient='index',columns=['WordFrequency'])\n",
        "uniqueWords.sort_values(by=['WordFrequency'], inplace=True, ascending=False)\n",
        "print(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))\n",
        "uniqueWords.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpChcGOTDkWn"
      },
      "source": [
        "# Trabajar solo con las palabras que se repiten màs de 50x \n",
        "\n",
        "uniqueWords=uniqueWords[uniqueWords['WordFrequency']>=50]\n",
        "print(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGo98ia0ISsy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud().generate(\" \".join(corpus))\n",
        "plt.imshow(wordcloud,interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEVf71UIfF4"
      },
      "source": [
        "#BAG OF WORDS\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = len(uniqueWords))\n",
        "\n",
        "#Create Bag of Words Model , here X represent bag of words\n",
        "X = cv.fit_transform(corpus).todense()\n",
        "y = ds_train['PG'].values\n",
        "y=y.astype('int') #There was a mistake in GNB given the fact that this column wasn't integer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhGiq4_AKeoH"
      },
      "source": [
        "#Split the train data set to train and test data\n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n",
        "print('Train Data splitted successfully')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8If842Jgcx"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "#Gaussian Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier_gnb = GaussianNB();\n",
        "classifier_gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Train data set results\n",
        "y_pred_gnb = classifier_gnb.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
        "cm_gnb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71NAdnJWMCyz"
      },
      "source": [
        "#MODEL ACCURACY \n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('GaussianNB Classifier Accuracy Score is {} for Train Data Set'.format(classifier_gnb.score(X_train, y_train)))\n",
        "print('GaussianNB Classifier Accuracy Score is {} for Test Data Set'.format(classifier_gnb.score(X_test, y_test)))\n",
        "print('GaussianNB Classifier F1 Score is {}'.format(f1_score(y_test, y_pred_gnb))) #does not run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXMC1T_1NR4L"
      },
      "source": [
        "#Fitting into test set\n",
        "X_testset = X_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AbPSB4NYcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f190b2fd-e636-420d-ae1f-5d149f11c1cf"
      },
      "source": [
        "#Predict data with classifier created in previous section\n",
        "y_test_pred_gnb = classifier_gnb.predict(X_testset)\n",
        "y_test_pred_gnb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    }
  ]
}